\documentclass[../note.tex]{subfiles}
\begin{document}

\section{Sequence and Series}
\subsection{Sequence}
\begin{definition}[Sequence]
	A sequence is a function $s: \mathbb{N} \to \mathbb{R}$. $s(n)$, the $n$th element of the sequence, is denoted as $s_n$.
	The domain of the sequence is all of the natural numbers, $\mathbb{N}$.
\end{definition}

\begin{definition}[Convergent and Divergent]
	A sequence, $s_n$, converges to $L$ if and only if $(\forall \epsilon > 0)(\exists N>0)(\forall n>N)(|s_n-L|<\epsilon)$. It is denoted as $\lim_{n\to \infty}s_n = L$.
	Otherwise, it diverges.
\end{definition}

\definition[Diverges to Infinity]{
	A sequence, $s_n$, diverges to positive infinity if and only if $(\forall M\in \bb{N})(\exists N>0)(\forall n>N)(s_n>M)$. It is denoted as $\lim_{n\to \infty}s_n = \infty$.

	Alternatively, the sequence diverges to negative infinity if and only if $(\forall M\in \bb{N})(\exists N>0)(\forall n>N)(s_n<M)$. It is denoted as $\lim_{n\to \infty}s_n = -\infty$.
}

\begin{definition}[Increasing and Decreasing Sequence(Monotone)]
	Let $s_n$ be a sequence. $s_n$ is increasing if and only if $s_{n+1} \geq s_n$ for all $n \in \mathbb{N}$. 
	$s_n$ is decreasing if and only if $s_{n+1} \leq s_n$ for all $n \in \mathbb{N}$. We we follow the nomenclature of \cite{Ross}.
\end{definition}

\begin{definition}[Limit of supremum \& infimum]\label{def:limiSupremum}
	For a sequence $(s_n)$, let $b_i$ denotes the supremum of $\{s_n|n>i\}$.
	If $(b_n)$ converges, the value it converges to is called the limit of supremum of $(s_n)$, and is denoted as $\mathcal{L}_s (s_n)$. 
	$(b_n)$ is called the supremum sequence.
	Similarly infimum sequence and limit of infimum are defined, and the later denoted as $\mathcal{L}_i(s_n)$. 
\end{definition}

\begin{remark}
	Notice supremum and infimum sequences are monotone.
\end{remark}
\begin{theorem} [Convergence and Limit of supremum \& infimum]\label{analysis:th:Convergence_and_limit_of_sup_in}
	A sequence $(s_n)$ converges if and only if $\mathcal{L}_s (s_n) = \mathcal{L}_i(s_n)$. 
	(Proposed Feb 8 2023, proved Feb 9)
\end{theorem}

\begin{proof}
We want to prove that $(\mathcal{L}_i (s_n) = \mathcal{L}_s (s_n)) \iff (s_n)$ converges. \\
Forward direction: We shall show that $\lim_{n\to \infty}(s_n)=\mathcal{L}_s(s_n) = \mathcal{L}_i (s_n) = \lambda$. $\forall \epsilon >0$, we know by our assumption that $(\exists N \in \mathbb{N})(\forall n>N)$ the set $\{s_n|n>N\}$ is bounded by $\lambda \pm \epsilon.$ This is the definition for the convergent sequence. 

We shall prove the contraposition of the backwards direction, i.e. $ (\mathcal{L}_i (s_n) \neq \mathcal{L}_s (s_n)) \rightarrow (s_n)$ diverges.
The contraposition can be proved by contradiction. 

Assuming $ (\lambda = \mathcal{L}_i (s_n) \neq \mathcal{L}_s (s_n))$ and $ (s_n)$ converges to $l$. S.D.U., let $\lambda > l$. 
Let $\epsilon = (\lambda - l)/2$. 
Since $(s_n)$ converges to $l$, there exists $N \in \mathbb{N}$ such that $\forall n>N$, $|s_n - l| < \epsilon$. 
However, we know that $\mathcal{L}_i (s_n) = \lambda$, which means that there exists $N'$ such that $\forall n>N'$ we have at least one element $s_i > \lambda - \epsilon$. Indeed $s_i - l>\epsilon$, contradicting with our assumption that $(s_n)$ converges. Thus we conclude the backwards direction is also true.
\end{proof}

\definition[Subsequence]{
	For a sequence $(s_n)$, its subsequence is a sequence $(s_{f(n)})$ where $f: \mathbb{N} \to \mathbb{N}$ is a strictly increasing function.
}

\theorem{
	A sequence converges if and only if all of its subsequence converges.
}

\begin{definition}[Cauchy Sequence]\cite{Ross}
	A sequence $(s_n)$ is a Cauchy Sequence iff $(\forall \epsilon > 0)(\exists N)(\forall n,m>N)(|s_n-s_m|<\epsilon)$
\end{definition}
\begin{theorem}
A sequence converges if and only if it is a Cauchy Sequence. 
\end{theorem}
\begin{remark}
We are to outline our proof of $(s_n)$ converges $\iff (s_n)$ is Cauchy Sequence. \\
The forward direction is obvious. To prove the backwards direction, notice: 1) All Cauchy Sequences are bounded; 2) the infimum and supremum sequence converge by monoteon convergence theorem; 3) They must converge to the same value;
4) By theorem \ref{analysis:th:Convergence_and_limit_of_sup_in} the sequence must converge. 
\end{remark}
\begin{remark}
	We can define a pseudo Cauchy Sequence to be sequence $(s_n)$ such that $(\forall \epsilon > 0)(\exists N)(\forall n>N)(|s_n-s_{n+1}|<\epsilon)$.
	Indeed all convergent sequence are pseudo Cauchy Sequence, but not all pseudo Cauchy Sequence are convergent. An example is the partial sum of harmonic series, i.e, $(\sum^{n}_{i=1}\frac{1}{i})$.
\end{remark}

\theorem[Squeeze Theorem]{
	Let $(s_n)$, $(b_n)$ and $(t_n)$ be sequences such that $(\forall n)(s_n \leq t_n \leq b_n)$. If $(s_n)$ and $(b_n)$ converges to the same limit, then $(t_n)$ converges to the same limit.
}

\theorem[Monotone Sequences]{
	An increasing sequence that is bounded above converges to its supremum. A decreasing sequence that is bounded below converges to its infimum.
}

\theorem[Limit Theorems of Sequences]{
	Let $(s_n)$ converges to $L_s$ and $(b_n)$ converges to $L_b$. Then:
	\begin{enumerate}
		\item Any convergent sequence are bounded.
		\item Any convergent sequence must have exactly one limit.
		\item $(s_n+b_n) $ converges to $L_s + L_b$.
		\item For any constant $c$, $(cs_n)$ converges to $cL_s$.
		\item $(s_nb_n)$ converges to $L_sL_b$.
		\item $(\frac{s_n}{b_n})$ converges to $\frac{L_s}{L_b}$ if $L_b \neq 0$.
	\end{enumerate}
}

\theorem[Bolzano-Weierstrass Theorem]{
	Every bounded sequence has a convergent subsequence. (Seems easy, hard to prove)
}

\subsection{Series}
\begin{definition}[Series]
	A series can be expressed as $ \sum^{\infty}_{k=1} a_k$. 
\end{definition}
\begin{definition}[Convergent and Divergent]
	Consider the seires :\\
	$(s_n)=\sum^{n}_{k=1} a_k$. $(s_n)$ is called the partial sum of the series. The series $\sum^{\infty}_{k=1}a_k $ converges if and only if its partial sum converges; otherwise it diverges. 
\end{definition}
\begin{example}
List of Convergent and Divergent series:
\begin{enumerate}
	\item Harmonic Series.
\end{enumerate}
\end{example}

\begin{definition}[Cauchy Criterion]A series befits Cauchy Criterion if and only if its partial sum is a Cauchy Sequence. \end{definition}

\begin{definition}[Absolute Convergent] A series $\sum^{\infty}_{k=1}a_k$ converges absolutely if and only if $\sum^{\infty}_{k=1}|a_k|$ converges. Otherwise it converges non-absolutely
\end{definition}
\begin{theorem}[Convergence Reveries]\label{th:ConvergenceReveries}
\ 
\begin{enumerate}
	\item For convergent series $\sum^{\infty}_{k=1} s_k$, $\sum^{\infty}_{k=1}s'_k$, and constant $c$, all of the following sequence converges:
		$\sum^{\infty}_{k=1}-s_k$ $\sum^{\infty}_{k=1}c\cdot s_k$, $\sum^{\infty}_{k=1}s_k+s'_k$, $\sum^{\infty}_{k+1}s_k\cdot s'_k$.\\
		In particular, $\sum^{\infty}_{k=1}\frac{1}{s_k}$ diverges.\\
		$\sum^{\infty}_{k=1}\frac{s_k}{s'_k}$ may diverge or converge.
	\item \label{th:ConvergenceReveries:en:absoluteconverge} \textbf{Absolute Convergent}:\\
		If a series converges absolutely, it converges. The converse is not true. 
	\item \textbf{Comparison Test}:\\
		For convergent series $\sum^{\infty}_{k=1}s_k$, if $|b_k|\leq s_k$ for all $k$, $\sum^{\infty}_{k=1}b_k$ converges.\\
		For divergent series $\sum^{\infty}_{k=1}d_k = \infty$, if $e_k\geq d_k$ for all $k$, $\sum^{\infty}_{k=1}e_k$ diverges. 
	\item \textbf{Ratio Test}:\\
		For series $\sum^{\infty}_{k=1}s_k$, let $d=\lim_{k \to \infty} |\frac{s_k}{s_{k-1}}|$.\\
		If $d<1$, the series converges absolutely.\\
		If $d>1$, the series diverges.\\
		If $d=1$, the series may converge or diverge.
	\item \textbf{Root Test}:\\
		For series $\sum^{\infty}_{k=1}s_k$, let $d=\lim_{k \to \infty} |s_k|^{1/k}$.\\
		If $d<1$, the series converges absolutely.\\
		If $d>1$, the series diverges.\\
		If $d=1$, the series may converge or diverge.
	\item \textbf{Alternating Series Test}: \label{AlternatingSeriesTest}\\
		For series in the form $\sum^{\infty}_{k=1}(-1)^{k}s_k$. If $s_k$ is decreasing and $\lim_{k\to \infty} s_k = 0$, the sereis converge. 
	\item \label{Cauchy_Condensation_Test} \textbf{Cauchy's Condenstation Test}:\\
		Consider series $\sum^{\infty}_{k=1}s_k$. If ${s_k}$ is decreasing and greater than zero, the seires converge if and only if $\sum^{\infty}_{k=1}s_{2^k}2^k $ converges. 
	\item \textbf{Integral Test}\label{IntegralTest}
		For $s_k>0$, the series $\sum^{\infty}_{k=1}s_k$ converge if and only if $\int_{a}^{\infty}S(k)dk$ converge for some constant $a$, provided $\forall k \in \mathbb{N}, S(k)=s_k$.
		(Proposed Feb 14 2023, modified and proved 16 Feb)
	\item \textbf{Raabe's Test}\label{Raabe's Test}
		For series $\sum^{\infty}_{k=1}s_k$, let $l= \displaystyle n\left(1-\frac{s_{n+1}}{s_n}\right)$. The series converge if $l>1$, diverges if $l<1$, and is inconclusive if $l=1$.

\end{enumerate}
\end{theorem}

\begin{hypothesis}[Inspired from the Integral Test]\label{hypothesis:Finite Riamenn Sum}
	If the finite integral, with some constant $a$, $\int_{a}^{\infty}f(k)dk$, converges for function $f$, $\lim_{n \to \infty}\sum^{n}_{i=0} f(\Delta x_ii + a)$ converge for $\Delta x_i \in \mathbb{R}$, provided $\{\Delta x_i\}$ is bounded. (Proposed 15 Feb 2023)
\end{hypothesis}

\begin{proof}

\begin{itemize}
	\item 
	To prove \ref{th:ConvergenceReveries:en:absoluteconverge} of theorem \ref{th:ConvergenceReveries},
	Consider the convergent series $\sum^{\infty}_{k=1}|a_k|$.
	Split $\sum^{\infty}_{k=1}a_k$ into $\sum^{\infty}_{k=1}p_k$ and $\sum^{\infty}_{k=1}n_k$, where $p_k, n_k$ are positive and negative, respectively. (We can safely ignore any 0)\\
	As $\sum^{\infty}_{k=1}p_k \leq \sum^{\infty}_{k=1}|a_k| $ and $\sum^{\infty}_{k=1}n_k \geq -\sum^{\infty}_{k=1}|a_k|$, both series are bounded. By Monotone convergence theorem, both serieses converge.
	Thus $\sum^{\infty}_{k=1}a_k$, as the sum of two convergent seires, must converge.

	\item
	Entry NO. \ref{Cauchy_Condensation_Test}, Cauchy's Condensation Test, has two directions: for decreasing and positive $s_k$, $\sum^{\infty}_{k=1}s_k$ converges $\iff \sum^{\infty}_{k=1}s_{2^k}2^k$ converges.

	To prove the forward direction, consider the convergent series:
	\[
		2\cdot \sum^{\infty}_{k=1}s_k =
		2\cdot s_1 + 2 \cdot (s_2+s_3) + 2 \cdot (s_4+s_5+s_6+s_7) \cdots
	\]

	And 
	\begin{equation} \label{eq:Forward_Proof_Cauchy_Condensation_Test}
		\sum^{\infty}_{k=1}s_{2^k}2^k = s_1+\underbrace{2\cdot s_2}_{<2\cdot s_1}
		+ \underbrace{4\cdot s_4}_{<2\cdot (s_2+s_3)}
		+ \underbrace{8\cdot s_8}_{<2\cdot (s_4+s_5+s_6+s_7)} + \cdots
	\end{equation}
	Thus by comparison test we conlude \eqref{eq:Forward_Proof_Cauchy_Condensation_Test} converges. 
	
	The backwards direction directly follows the comparison test as\\ $ \sum^{\infty}_{k=1}s_{2^k}2^k \geq \sum^{\infty}_{k=1}s_k$.
	\item 
		Here we present an informal proof fo \ref{th:ConvergenceReveries}.\ref{IntegralTest}, the integral test with an extra restriction that the function is strictly decreasing. (15 Feb 2023)

		Consider the function $S$ with the property $\int_{a}^{\infty}S(k)dk$ converges for some constant $a$ and its correspondent series 
		$\sum^{\infty}_{k=1}S(k)$. 
		Consider the function $\sigma (x) = S(x-1)$ $\int_{a}^{\infty}\sigma(k)dk$ converges, and is greater than $\sum^{\infty}_{k=\left\lceil{a}\right\rceil}s_k$(as the function is strictly decreasing), thus by comparison test it converges,
		thus $\sum^{\infty}_{k=1}s_k$, as the sum of a convergent series and a constant also converge.
\end{itemize}
\end{proof}

\subsection{Interestring Sequences and Series}
\paragraph{Sequences}
\paragraph{Series}
	\small
\begin{enumerate}
	\item $\sum \frac{\sqrt{n+1}-\sqrt{n}}{n^k}$ diverges for $k\leq \frac{1}{2}$, converges otherwise.
	\item $\sum \frac{1}{n\log{n}}$ diverges.
	\item $\sum(\frac{1}{n}-\frac{1}{n+1})$ converges. 
	\item $\sum^{\infty}_{n=1}\frac{n}{2^n}=1$  
\end{enumerate}


\subsection{Decimal Expansion}



\end{document}
