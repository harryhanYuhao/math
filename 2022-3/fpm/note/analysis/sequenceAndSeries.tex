\documentclass[../note.tex]{subfiles}
\begin{document}

\section{Sequence and Series}
\subsection{Sequence}
\begin{definition}[Sequence]
\end{definition}

\begin{definition}[Convergent and Divergent]
\end{definition}

\begin{definition}[Increasing and Decreasing Sequence(Monotone)]
\end{definition}

\begin{definition}[Limit of supremum \& infimum]\label{def:limiSupremum}
	For a sequence $(s_n)$, let $b_i$ denotes the supremum of $\{s_n|n>i\}$.
	If $(b_n)$ converges, the value it converges to is called the limit of supremum of $(s_n)$, and is denoted as $\mathcal{L}_s (s_n)$. 
	$(b_n)$ is called the supremum sequence.
	Similarly infimum sequence and limit of infimum are defined, and the later denoted as $\mathcal{L}_i(s_n)$. 
\end{definition}

\begin{remark}
	Notice supremum and infimum sequences are monotone.
\end{remark}
\begin{theorem} [Convergence and Limit of supremum \& infimum]\label{analysis:th:Convergence_and_limit_of_sup_in}
	A sequence $(s_n)$ converges if and only if $\mathcal{L}_s (s_n) = \mathcal{L}_i(s_n)$. 
	(Proposed Feb 8 2023, proved Feb 9)
\end{theorem}

\begin{proof}
We want to prove that $(\mathcal{L}_i (s_n) = \mathcal{L}_s (s_n)) \iff (s_n)$ converges. \\
Forward direction: We shall show that $\lim_{n\to \infty}(s_n)=\mathcal{L}_s(s_n) = \mathcal{L}_i (s_n) = \lambda$. $\forall \epsilon >0$, we know by our assumption that $(\exists N \in \mathbb{N})(\forall n>N)$ the set $\{s_n|n>N\}$ is bounded by $\lambda \pm \epsilon.$ This is the definition for the convergent sequence. 

We shall prove the contraposition of the backwards direction, i.e. $ (\mathcal{L}_i (s_n) \neq \mathcal{L}_s (s_n)) \rightarrow (s_n)$ diverges.
The contraposition can be proved by contradiction. 

Assuming $ (\lambda = \mathcal{L}_i (s_n) \neq \mathcal{L}_s (s_n))$ and $ (s_n)$ converges to $l$. S.D.U., let $\lambda > l$. 
Let $\epsilon = (\lambda - l)/2$. 
Since $(s_n)$ converges to $l$, there exists $N \in \mathbb{N}$ such that $\forall n>N$, $|s_n - l| < \epsilon$. 
However, we know that $\mathcal{L}_i (s_n) = \lambda$, which means that there exists $N'$ such that $\forall n>N'$ we have at least one element $s_i > \lambda - \epsilon$. Indeed $s_i - l>\epsilon$, contradicting with our assumption that $(s_n)$ converges. Thus we conclude the backwards direction is also true.
\end{proof}

\begin{definition}[Cauchy Sequence]\cite{Ross}
	A sequence $(s_n)$ is a Cauchy Sequence iff $(\forall \epsilon > 0)(\exists N)(\forall n,m>N)(|s_n-s_m|<\epsilon)$
\end{definition}
\begin{theorem}
A sequence converges if and only if it is a Cauchy Sequence. 
\end{theorem}
\begin{remark}
We are to outline our proof of $(s_n)$ converges $\iff (s_n)$ is Cauchy Sequence. \\
The forward direction is obvious. To prove the backwards direction, notice: 1) All Cauchy Sequences are bounded; 2) the infimum and supremum sequence converge by monoteon convergence theorem; 3) They must converge to the same value;
4) By theorem \ref{analysis:th:Convergence_and_limit_of_sup_in} the sequence must converge. 
\end{remark}
\begin{remark}
	We can define a pseudo Cauchy Sequence to be sequence $(s_n)$ such that $(\forall \epsilon > 0)(\exists N)(\forall n>N)(|s_n-s_{n+1}|<\epsilon)$.
	Indeed all convergent sequence are pseudo Cauchy Sequence, but not all pseudo Cauchy Sequence are convergent. An example is the partial sum of harmonic series, i.e, $(\sum^{n}_{i=1}\frac{1}{i})$.
\end{remark}

\subsection{Series}
\begin{definition}[Series]
	A series can be expressed as $ \sum^{\infty}_{k=1} a_k$. 
\end{definition}
\begin{definition}[Convergent and Divergent]
	Consider the seires :\\
	$(s_n)=\sum^{n}_{k=1} a_k$. $(s_n)$ is called the partial sum of the series. The series $\sum^{\infty}_{k=1}a_k $ converges if and only if its partial sum converges; otherwise it diverges. 
\end{definition}
\begin{example}
List of Convergent and Divergent series:
\begin{enumerate}
	\item Harmonic Series.
\end{enumerate}
\end{example}

\begin{definition}[Cauchy Criterion]A series befits Cauchy Criterion if and only if its partial sum is a Cauchy Sequence. \end{definition}

\begin{definition}[Absolute Convergent] A series $\sum^{\infty}_{k=1}a_k$ converges absolutely if and only if $\sum^{\infty}_{k=1}|a_k|$ converges. Otherwise it converges non-absolutely
\end{definition}
\begin{theorem}[Convergence Reveries]\label{th:ConvergenceReveries}
\ 
\begin{enumerate}
	\item For convergent series $\sum^{\infty}_{k=1} s_k$, $\sum^{\infty}_{k=1}s'_k$, and constant $c$, all of the following sequence converges:
		$\sum^{\infty}_{k=1}-s_k$ $\sum^{\infty}_{k=1}c\cdot s_k$, $\sum^{\infty}_{k=1}s_k+s'_k$, $\sum^{\infty}_{k+1}s_k\cdot s'_k$.\\
		In particular, $\sum^{\infty}_{k=1}\frac{1}{s_k}$ diverges.\\
		$\sum^{\infty}_{k=1}\frac{s_k}{s'_k}$ may diverge or converge.
	\item \label{th:ConvergenceReveries:en:absoluteconverge} \textbf{Absolute Convergent}:\\
		If a series converges absolutely, it converges. The converse is not true. 
	\item \textbf{Comparison Test}:\\
		For convergent series $\sum^{\infty}_{k=1}s_k$, if $|b_k|\leq s_k$ for all $k$, $\sum^{\infty}_{k=1}b_k$ converges.\\
		For divergent series $\sum^{\infty}_{k=1}d_k = \infty$, if $e_k\geq d_k$ for all $k$, $\sum^{\infty}_{k=1}e_k$ diverges. 
		If $\sum^{\infty}_{k=1}a_k $ and $\sum^{\infty}_{k=1}b_k$ converges, the followings also converge:
		$\sum^{\infty}_{k=1}(a_k+b_k) $, $\sum^{\infty}_{k=1}(a_k-b_k) $ $\sum^{\infty}_{k=1}(a_k\cdot b_k)$
	\item \textbf{Ratio Test}:\\
		For series $\sum^{\infty}_{k=1}s_k$, let $d=\lim_{k \to \infty} |\frac{s_k}{s_{k-1}}|$.\\
		If $d<1$, the series converges absolutely.\\
		If $d>1$, the series diverges.\\
		If $d=1$, the series may converge or diverge.
	\item \textbf{Root Test}:\\
		For series $\sum^{\infty}_{k=1}s_k$, let $d=\lim_{k \to \infty} |(s_k)^{1/k}|$.\\
		If $d<1$, the series converges absolutely.\\
		If $d>1$, the series diverges.\\
		If $d=1$, the series may converge or diverge.
	\item \textbf{Alternating Series Test}:\\
		For series in the form $\sum^{\infty}_{k=1}(-1)^{k}s_k$. If $s_k$ is decreasing and $\lim_{k\to \infty} s_k = 0$, the sereis converge. (Copied from textbook on 14 Feb 2023, not proved.)
	\item \label{Cauchy_Condensation_Test} \textbf{Cauchy's Condenstation Test}:\\
		Consider series $\sum^{\infty}_{k=1}s_k$. If ${s_k}$ is decreasing and greater than zero, the seires converge if and only if $\sum^{\infty}_{k=1}s_{2^k}2^k $ converges. 
	\item \textbf{Integral Test}\label{IntegralTest}
		For $s_k>0$, the series $\sum^{\infty}_{k=1}s_k$ converge if and only if $\int_{a}^{\infty}S(k)dk$ converge for some constant $a$, provided $\forall k \in \mathbb{N}, S(k)=s_k$.
		(Proposed Feb 14 2023)

\end{enumerate}
\end{theorem}

\begin{hypothesis}[Inspired from the Integral Test]
	If the finite integral, with some constant $a$, $\int_{a}^{\infty}f(k)dk$, converges for function $f$, $\lim_{n \to \infty}\sum^{n}_{i=0} f(\Delta x_ii + a)$ converge for $\Delta x_i \in \mathbb{R}$, provided $\{\Delta x_i\}$ is bounded. (Proposed 15 Feb 2023)
\end{hypothesis}

\begin{proof}

\begin{itemize}
	\item 
	To prove \ref{th:ConvergenceReveries:en:absoluteconverge} of theorem \ref{th:ConvergenceReveries},
	Consider the convergent series $\sum^{\infty}_{k=1}|a_k|$.
	Split $\sum^{\infty}_{k=1}a_k$ into $\sum^{\infty}_{k=1}p_k$ and $\sum^{\infty}_{k=1}n_k$, where $p_k, n_k$ are positive and negative, respectively. (We can safely ignore any 0)\\
	As $\sum^{\infty}_{k=1}p_k \leq \sum^{\infty}_{k=1}|a_k| $ and $\sum^{\infty}_{k=1}n_k \geq -\sum^{\infty}_{k=1}|a_k|$, both series are bounded. By Monotone convergence theorem, both serieses converge.
	Thus $\sum^{\infty}_{k=1}a_k$, as the sum of two convergent seires, must converge.

	\item
	Entry NO. \ref{Cauchy_Condensation_Test}, Cauchy's Condensation Test, has two directions: for decreasing and positive $s_k$, $\sum^{\infty}_{k=1}s_k$ converges $\iff \sum^{\infty}_{k=1}s_{2^k}2^k$ converges.

	To prove the forward direction, consider the convergent series:
	\[
		2\cdot \sum^{\infty}_{k=1}s_k =
		2\cdot s_1 + 2 \cdot (s_2+s_3) + 2 \cdot (s_4+s_5+s_6+s_7) \cdots
	\]

	And 
	\begin{equation} \label{eq:Forward_Proof_Cauchy_Condensation_Test}
		\sum^{\infty}_{k=1}s_{2^k}2^k = s_1+\underbrace{2\cdot s_2}_{<2\cdot s_1}
		+ \underbrace{4\cdot s_4}_{<2\cdot (s_2+s_3)}
		+ \underbrace{8\cdot s_8}_{<2\cdot (s_4+s_5+s_6+s_7)} + \cdots
	\end{equation}
	Thus by comparison test we conlude \eqref{eq:Forward_Proof_Cauchy_Condensation_Test} converges. 
	
	The backwards direction directly follows the comparison test as\\ $ \sum^{\infty}_{k=1}s_{2^k}2^k \geq \sum^{\infty}_{k=1}s_k$.
	\item 
		Here we present an informal proof fo \ref{th:ConvergenceReveries}.\ref{IntegralTest}, the integral test with an extra restriction that the function is strictly decreasing. (15 Feb 2023)

		Consider the function $S$ with the property $\int_{a}^{\infty}S(k)dk$ converges for some constant $a$ and its correspondent series 
		$\sum^{\infty}_{k=1}S(k)$. 
		Consider the function $\sigma (x) = S(x-1)$ $\int_{a}^{\infty}\sigma(k)dk$ converges, and is greater than $\sum^{\infty}_{k=\left\lceil{a}\right\rceil}s_k$(as the function is strictly decreasing), thus by comparison test it converges,
		thus $\sum^{\infty}_{k=1}s_k$, as the sum of a convergent series and a constant also converge.
\end{itemize}
\end{proof}


\end{document}
