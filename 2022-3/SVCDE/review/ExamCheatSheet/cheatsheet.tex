\documentclass[9pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{blindtext, titlesec, amsthm, thmtools, amsmath, amsfonts, scalerel, amssymb, graphicx, titlesec}
\usepackage{geometry}
\geometry{a4paper, total={170mm,257mm}, left=10mm, right=10mm, bottom=15mm, top=10mm}

\usepackage{setspace}
\singlespacing
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{hypothesis}{Hypothesis}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]

\begin{document}

\section{Vectors and Geometry}

\begin{theorem}[Vector Computation]
Basics computation rules for vectors:
	\begin{enumerate}
		\item $\underline{a} \cdot \underline{b} = \cos{\theta}  | \underline{a}| | \underline{b}| $
		\item $| \underline{a} \times \underline{b} | = \sin{\theta}  | \underline{a}| | \underline{b}| $
		\item $ \underline{a} \times \underline{b} = - \underline{b} \times \underline{a} $
		\item $ \underline{a} \cdot (\underline{b} \times \underline{c}) = ( \underline{a}\times \underline{b}) \cdot \underline{c} $
		\item $ \underline{a} \times (\underline{b} \times \underline{c}) = (\underline{a} \cdot \underline{c}) \underline{b} - (\underline{a} \cdot \underline{b}) \underline{c} $
		\item $ \underline{a} \cdot ( \underline{b} \times \underline{c}) = \begin{vmatrix}
			a_1 & a_2 & a_3 \\
			b_1 & b_2 & b_3 \\
			c_1 & c_2 & c_3
			\end{vmatrix}, $ moreover, this equal to the area of the parallelepiped specified by the vectors $ \underline{a}, \underline{b}, \underline{c} $
 	\end{enumerate}
\end{theorem}

\begin{theorem}[Arc Length]
	The length of the curve traced out by the postion vector function $ \underline{r}(t)=x(t) \underline{i} + y(t) \underline{j} + z(t) \underline{k} $ over the interval $ a \leq t \leq b$ is 
$
s(t)= \int_a^b | \underline{r}'(t) | dt
$

\end{theorem}
\begin{definition}[Unit Tangent Vector]
	
	For vector function $ \underline{r}(t) $, its unit tangent vector is $ \underline{T} =\frac{ \underline{r'}(t)}{| \underline{r'}(t)|} $
\end{definition}

\begin{definition}[Curvature]
	For vector function $ \underline{r}(t) $, its curvature is
	$\kappa  = \frac{| \underline{T'}(t)|}{| \underline{r'}(t)|}= \frac{ |\underline{r}'(t) \times \underline{r}''(t)|}{| \underline{r}'(t)|^3} $
	
\end{definition}

\begin{definition}[Normal and binormal Vector]
	For vector function $ \underline{r}(t) $, its unit tangent vector is $ \underline{T} = \frac{ \underline{r'}(t)}{| \underline{r'}(t)|} $
	, and its unit normal vector is $ \underline{N}(t) = \frac{ \underline{T'}(t)}{| \underline{T'}(t)|}$, and its binormal vector is $ \underline{T}(t) \times \underline{N}(t) $
\end{definition}
\section{Partial Derivatives}

\begin{definition}[Differentiability]
	Let $z = f(x,y), \Delta z = f(a+\Delta x,b+\Delta y) - f(x,y) $, then the function is differentialbe if and only if $(\epsilon _1, \epsilon _2) \to (0, 0)$ as  $(\Delta x, \Delta y) \to (0, 0)$, where $ \epsilon _1, \epsilon _2 $ are defined as 
$	
\Delta z = f_x(a,b) \Delta x + f_y(a,b) \Delta y + \epsilon _1 \Delta x + \epsilon _2 \Delta y
$

Moreover, it can be shown that $z$ is differentiable at $(a,b)$ if and only if the partial derivative $f_x, f_y$ exisits and are continuous near $(a,b)$
\end{definition}

\begin{definition}[Differentials]
	$w = f(x,y,z)$ the total differential $dw$ is defined as 
	$
dw = \frac{\partial w}{\partial x} dx + \frac{\partial w}{\partial y} dy + \frac{\partial w}{\partial z} d z
	$
	
\end{definition}
\begin{theorem}[Chain Rule]
Suppose $z = f(x, y)$ and $ x = g(t)$, $y = h(t)$ then 
$
\frac{\partial z}{\partial t} = \frac{\partial z}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial z}{\partial y} \frac{\partial y}{\partial t}
$

Suppose $z = f(x, y)$ and $ x = g(t, u)$, $y = h(t, u)$ then
$
\frac{\partial z}{\partial t} = \frac{\partial z}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial z}{\partial y} \frac{\partial y}{\partial t}, 
\frac{\partial z}{\partial u}
= \frac{\partial z}{\partial x} \frac{\partial x}{\partial u} + \frac{\partial z}{\partial y} \frac{\partial y}{\partial u}
$

\end{theorem}

\begin{theorem}[Implicit Differentiation]
	Suppose $F(x,y,z) = 0$, and $z$ is a implicit funtion of $x$, then:
$	
		\frac{\partial z}{\partial x} = - \frac{F_x}{F_z}, 
$
\end{theorem}
\begin{definition}[Gradient]
	Let $f(x,y)$ be a function of two variables, then the gradient of $f$ is defined as a vector function: 
	$
\nabla f(x,y) = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right)
	$
\end{definition}

\begin{definition}[Directional Derivative]
	For funtion $f(x,y)$, the directional derivative at $(a,b)$ at the direction of the unit vector $\underline{u} = (u_1, u_2)$ in $x-y$ plane is:
$
	\nabla f \cdot \underline{u} = f_x(a,b) u_1 + f_y(a,b) u_2
$
. Note it is a constant.
\end{definition}
\begin{theorem}[Maximun Directional Derivative]
	Suppose $f(x,y)$ is differentiable at $(a,b)$, then the directional derivative of $f$ at $(a,b)$ in the direction of $u$ is maximized if and only if $u$ is parallel to the gradient of $f$ at $(a,b)$.
\end{theorem}
\subsection{Maximum and Minimum}
\begin{definition}[Critical Point]
	Let $f(x,y)$ be a function of two variables, then $(a,b)$ is a critical point of $f$ if $f_x(a)=0$ and$ f_y=(b)0,$ or one of its partialderivative does not exist.
\end{definition}

\begin{theorem}[Second Derivative Test]
Suppose the secound partial derivative of $f(x,y)$ exists at $(a,b)$ and is continous around it, and $f_x(a,b)=f_y(a,b) = 0$
Let 
\[
D(a,b) = f_{xx}(a,b) f_{yy}(a,b) - f_{xy}(a,b)^2 = \left| \begin{matrix} f_{xx}(a,b) & f_{xy}(a,b) \\ f_{xy}(a,b) & f_{yy}(a,b) \end{matrix} \right|
\]
Then 
\begin{enumerate}
	\item If $D(a,b) > 0$ and $f_{xx} > 0$, then $(a,b)$ is a local minimum of $f(x,y)$
	\item If $D(a,b) > 0$ and $f_{xx}<0$, then $(a,b)$ is a local maximum of $f(x,y)$
	\item If $D(a,b) < 0$, then $(a,b)$ is a saddle point of $f(x,y)$
	\item If $D(a,b) = 0$, the test is inconclusive
\end{enumerate}
\end{theorem}
\subsubsection{Function of Three Variables}
\begin{theorem}[Second Derivative Test]
Suppose the secound partial derivative of $f(x,y,z)$ exists at $(a,b,c)$ and is continous around it, and $f_x(a,b,c)=f_y(a,b,c)=f_z(a,b,c) = 0$
Let 
\[
	D_1(a, b, c) = f_{xx}(a,b,c)
\]
\[
	D_2(a, b, c) = \begin{vmatrix} f_{xx}(a,b,c) & f_{xy}(a,b,c) \\ f_{xy}(a,b,c) & f_{yy}(a,b,c) \end{vmatrix}
\]
\[
	D_3(a, b, c) = \begin{vmatrix} f_{xx}(a,b,c) & f_{xy}(a,b,c) & f_{xz}(a,b,c) \\ f_{xy}(a,b,c) & f_{yy}(a,b,c) & f_{yz}(a,b,c) \\ f_{xz}(a,b,c) & f_{yz}(a,b,c) & f_{zz}(a,b,c) \end{vmatrix}
\]
Then 
\begin{enumerate}
	\item If $D_1(a,b,c) > 0$ and $D_2(a,b,c) > 0$ and $D_3(a,b,c) > 0$, then $(a,b,c)$ is a local minimum of $f(x,y,z)$
	\item If $D_1(a,b,c) < 0$ and $D_2(a,b,c) > 0$ and $D_3(a,b,c) < 0$, then $(a,b,c)$ is a local maximum of $f(x,y,z)$
	\item If $D_3 \neq 0$ and $(a,b,c)$ is neither a maxium nor a minimum, then $(a,b,c)$ is a saddle point of $f(x,y,z)$
	\item If $D_3(a,b,c) = 0$, the test is inconclusive

\end{enumerate}
\end{theorem}
\subsubsection{Lagrange Multiplier}

\begin{theorem}[Lagrange Multiplier]
	Let $f(x,y)$ be a function of two variables and differential along the smooth curve $g(x,y) = 0$ where $\nabla g (x,y) \neq 0.$ 
	if $f$ attains extremum along $g = 0$ at $(a,b)$, then there exists $\lambda \in \mathbb{R}$ such that
	$
		\nabla f (a,b)= - \lambda \nabla g(a, b)
	$
\end{theorem}
\section{Multiple Integrals}

\subsection{Change of Variables}
\begin{theorem}[Double Integrals to Polar Coordinates]
For $(x, y)$ in Cartesian coordinates. If the area $R$ is given by $r_0 \leq r \leq r_1, \theta_0 \leq \theta \leq \theta_1$, then:
$
	\iint_R f(x,y) dA = \iint_D f(r\cos{\theta}, r\sin{\theta}) r dr d\theta
$
\end{theorem}

\begin{theorem}[Triple Integrals to Cylindrical Coordinates]
Recall the cylindrical coordinate $(r, \theta, z)$ is defined by: $r^2 = x^2+y^2, x = r\cos{\theta}, y = r\sin{\theta}$
If the volume $V$ is given by $r_0 \leq r \leq r_1, \theta_0 \leq \theta \leq \theta_1, z_0 \leq z \leq z_1$, then:
$
	\iiint_V f(x,y,z) dV = \iiint_D f(r\cos{\theta}, r\sin{\theta}, z) r dz dr d\theta
$
Notice the extra $r$ in the integrand.
\end{theorem}

\begin{theorem}[Triple Integral to Spherical Coordinates]
Recall the spherical coordinate
$(r, \theta, \phi)$
is defined by: $r^2 = x^2+y^2+z^2, x = r\sin{\phi}\cos{\theta}, y = r\sin{\theta}\sin{\phi}, z = r\cos{\phi}$

If the volume $V$ is given by $r_0 \leq r \leq r_1, \theta_0 \leq \theta \leq \theta_1, \phi_0 \leq \phi \leq \phi_1$, then:

$
	\iiint_V f(x,y,z) dV = \iiint_D f(r\sin{\phi}\cos{\theta}, r\sin{\theta}\sin{\phi}, r\cos{\phi}) r^2 \sin{\phi} dr d\theta d\phi
$

Notice the extra $r^2 \sin{\phi}$ in the integrand.
\end{theorem}


\subsubsection{Change of Variable in General: Jacobian}
\begin{definition}[Jacobian]
The following only applies to bijective transformation.
Two D transformation from $u,v$ plane to $x,y$ plane is defined as $x = x(u,v), y = y(u, v)$
Three D transformation from $u,v,w$ plane to $x,y,z$ plane is defined as $x=x(u,v,w), y = y(u,v,w), z = z(u,v,w)$. There respective jacobians are:
	\[
		J_2 = \frac{\partial x \partial y}{\partial u \partial v} 
		=\begin{vmatrix}
			\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
			\frac{\strut\partial y}{\partial u} & \frac{\partial y}{\partial v}
		\end{vmatrix}
,
	J_3 = \frac{\partial x \partial y \partial z}{\partial u \partial v \partial w}
=	\begin{vmatrix}
		\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} & \frac{\partial x}{\partial w} \\
		\frac{\strut\partial y}{\partial u} & \frac{\partial y}{\partial v} & \frac{\partial y}{\partial w} \\
		\frac{\strut\partial z}{\partial u} & \frac{\partial z}{\partial v} & \frac{\partial z}{\partial w}
	\end{vmatrix}
	\]
\end{definition}

\begin{theorem}[Change of Variable in General]
The following only applies to bijective transformation.
	For the transformation from $u,v$ plane to $x,y$ plane:
$
	\iint_D f(x,y) dA = \iint_D g(u,v) |J_2| du dv
$
Where $g(u,v) = f(x(u,v), y(u,v))$.

For transformation $u,v,w$ plane to $x,y,z$ plane:
$
	\iiint_V f(x,y,z) dV = \iiint_V g(u,v,w) |J_3| du dv dw
$
Where $g(u,v,w) = f(x(u,v,w), y(u,v,w), z(u,v,w))$.

Notice the Jacobian is taken as absolute value.
\end{theorem}

\section{Vector Calculus}

\begin{definition}[Vector Field]
A vector field is a function 
$\underline{F}: \mathbb{R}^n \rightarrow \mathbb{R}^n$.
The input and output are $n$ dimensional vectors.
\end{definition}

\begin{theorem}[Conservative Vector Fields]
A vector field $\underline{F}$ is conservative if there exists a function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ such that
$\underline{\nabla} f = \underline{F}$.
Moreoever, the Vector field $ \underline{F}(x,y)$
	is conservative, provided that it is defined on all 
	$ \mathbb{R}^2,$ if and only if
	$
		\frac{\partial P}{\partial y} = \frac{\partial Q}{\partial x}
	$

For vector field $ \underline{G}(x,y,z)$, provided it is defined on all $ \mathbb{R}^3$, it is conservative if and only if:
$
	\underline{\nabla} \times \underline{G} = 0
$
\end{theorem}

\begin{definition}[Curl and Divergence]
The curl and divergence of a vector field $\underline{F} = F_x \underline{i} +  F_y \underline{j} + F_z \underline{k}$ is defined as below respectively:
\[
	\underline{\nabla} \times \underline{F} = 
	\begin{vmatrix}
		\underline{i} & \strut\underline{j} & \underline{k} \\
		\frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\
		F_x & F_y & F_z
	\end{vmatrix}
,
	\underline{\nabla} \cdot \underline{F} = \frac{\partial F_x}{\partial x} + \frac{\partial F_y}{\partial y} + \frac{\partial F_z}{\partial z}
\]

	Two facts about curl and divergence:
		1. \text{div} (\text{curl} $\underline{F}$) = 0; 2. \text{curl} ($\nabla f) = 0$

\end{definition}
\subsection{Line Integral and Surface Integral}
\begin{definition}[Line Integral]

If $f$ is a function of three variable, the line integral along the curve $\gamma$ is defined as:
$
	\int_\gamma f(x,y,z) ds 
	= \int_{t_0} ^{t_1} f(x(t), y(t), z(t)) \sqrt{ \frac{dx}{dt}^2 + \frac{dy}{dt}^2 + \frac{dz}{dt}^2 } dt
$
Where $\gamma$ is parametrized by $x=x(t), y=y(t), z=z(t),$ from $t_0$ to $t_1$.
Moreover
$
	\int_\gamma F dx = \int_{t_0} ^{t_1} F(x(t), y(t), z(t)) \frac{dx}{dt} dt
$
\end{definition}

\begin{definition}[Line Integral of a Vector Field]
	The line integral of a vector field $\underline{F}$ along a curve $\gamma$ is described by the vector function $ \underline{r}$ is defined as:
$
	\int_\gamma \underline{F} \cdot d\underline{r} 
	=	 \int_{\gamma} \underline{F} (\underline{r}) \cdot \underline{r}'dt
	= \int_{\gamma} P_x dx + P_y dy + P_z dz
$
Where $ \underline{F} = \underline{i} P_x + \underline{j} P_y +  \underline{k} P_z$
\end{definition}

\begin{theorem}[Fundamental Theorem of Line Integral]
If $\underline{F}$ is conservative, and $ \underline{\nabla}f = \underline{F}$. 
Let $\gamma$ be a curve which starts from $(x_0, y_0)$ and ends at $(x_1, y_1)$.
Then
$
\int _\gamma \underline{F} \cdot d\underline{r} = f(x_1, y_1) - f(x_0, y_0)
$
\end{theorem}

\begin{theorem}[Surface Area]
The surface area of a parametrized surface $\underline{r}(u,v) = \underline{i}x(u,v) + \underline{j}y(u,v) + \underline{k}z(u,v)$, for $(u,v) \in \mathbb{D}$ is:
$
	\iint_{\mathbb{D}} | \underline{r}_u \times \underline{r}_v | dA
$
Where $ \underline{r}_u = \frac{\partial \underline{r}}{\partial u}$ 
and $ \underline{r}_v = \frac{\partial \underline{r}}{\partial v}$. 

Notice $ \underline{r}_u $ and $ \underline{r}_v $ are tangent vectors to the surface, and $||$ operator returns their magnitude.

Moreover when $z$ is a function of $x, y$:
Surface is paramatrized as $ \underline{r} = \underline{i}x+ \underline{j}y+ \underline{k} f(x,y),$ and the surface area of is 
$
	\iint_{ \mathbb{D}} \sqrt{1 + \left( \frac{\partial f}{\partial x} \right)^2 + \left( \frac{\partial f}{\partial y} \right)^2} dA
$
\end{theorem}


\begin{definition}[Surface Integral]
	Consider function $f(x,y,z)$ defined on the surface $ \mathbb{S}$ which is described by the vector function: 
	\[
		\underline{r}(u,v) = \underline{i}x(u,v) + \underline{j}y(u,v) + \underline{k}z(u,v), (u, v) \in \mathbb{D}
	\]
	Then the surface integral of $f$ over the surface $\mathbb{S}$ is defined as:
	\[
		\iint_{\mathbb{S}} f(x,y,z) dS
	=
		\iint_{\mathbb{D}} f( \underline{r}(u,v)) | \underline{r}_u \times \underline{r}_v | dA
	\]
Where $ \underline{r}_u = \frac{\partial \underline{r}}{\partial u}$
$ \underline{r}_v = \frac{\partial \underline{r}}{\partial v}$,
and$f( \underline{r}(u,v)) = f(x(u,v), y(u,v), z(u,v))$
\end{definition}

\begin{definition}[Oriented Surface]
	Let $\underline{r}(u,v)$ be a vector function on $\mathbb{D}$ that describes the surface $S$.
	$ \frac{\underline{r}_u \times \underline{r}_v}{| \underline{r}_u \times \underline{r}_v|}$ is its unit normal vector to $S$.

	Moreover, for surface $z = f(x,y)$ the unit normal vector is:
	$
		\frac{-\underline{i} \frac{\partial f}{\partial x} - \underline{j} \frac{\partial f}{\partial y} + \underline{k}}
		{\sqrt{1 + \left( \frac{\partial f}{\partial x} \right)^2 + \left( \frac{\partial f}{\partial y} \right)^2}}
	$
\end{definition}
\begin{definition}[Surface Integral Over Vector Field]
	Let $\underline{F}(x,y,z)$ be a vector field defined on the surface $S$.
	Then the surface integral of $\underline{F}$ over $S$ is defined as:
$
	\iint_{\mathbb{D}} \underline{F}(\underline{r}(u,v)) \cdot \left( \underline{r}_u \times \underline{r}_v \right)dA
$	
. Moreover, for surface $z = g(x,y)$, we have:
$
	\underline{F} \cdot | \underline{r}_u \times \underline{r}_v| 
=
	(F_x \underline{i} + F_y \underline{j} + F_z \underline{k}) \cdot \left( - \frac{ \partial g}{ \partial x} \underline{i} - \frac{ \partial g}{ \partial y} \underline{j} + \underline{k} \right)
$
And the surface integral is:
$
\iint _{ \mathbb{D}} - F_x \frac{ \partial g}{ \partial x} - F_y \frac{ \partial g}{ \partial y} + F_z dA.
$
\end{definition}

\begin{theorem}[Green's Theorem]
	Let $\gamma$ be positively oriented curve, simply close, and piecewise smooth curve in $x,y$ plane.
	Let $ \Gamma$ be the region bounded by $\gamma$.
	If $ P(x,y)$ and $Q(x,y)$ are continuous functions on $\Gamma$, then
$
	\int _{\gamma} P dx + Qdy = \iint _{\Gamma} \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} dA
$

\end{theorem}

\begin{theorem}[Stokes' Theorem]
	Let $\gamma$ be positively oriented, simply close, and piecewise smooth curve in $x,y$ plane.
	Let $ \Gamma$ be the region bounded by $\gamma$.
	If $ \underline{F}(x,y, z)$ is a continuous vector field on $\Gamma$, then
$
	\iint _{\Gamma} \underline{\nabla} \times \underline{F} \cdot d\underline{S} = \int _{\gamma} \underline{F} \cdot d\underline{r}
$

\end{theorem}

\begin{theorem}[Divergence Theorem]
Let $E$ be a simple solid region bounded by a positively oriented surface (outward) $\mathbb{S}$.
Let $ \underline{F}: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ be a vector field that has continuous partial derivative in $E$, then:
$
	\iint _{\mathbb{S}} \underline{F} \cdot d\underline{S} = \iiint _{E} \underline{\nabla} \cdot \underline{F} dV
$

\end{theorem}

\section{Differential Equations}
\begin{definition}[Classification of Differential Equation]
Classification of Differential Equation.
	\begin{enumerate}
		\item Ordinary differential equation (ODE) involves only differentiation of one variable
		\item Partial differential equation (PDE) involves differentiation of more than one variable.
		\item Order of differential equation is the highest order of differentiation in the equation.
	\end{enumerate}
The $n^{\text{th}}$ order linear differential equation has the general form:
\[
	\frac{d^n y}{dx^n} + a_1 \frac{d^{n-1} y}{dx^{n-1}} + a_2 \frac{d^{n-2} y}{dx^{n-2}} + \cdots + a_{n-1} \frac{d y}{dx} + a_n y = a_0(x)
\]
Where $a_i$ asre functions of $x$. 
The equation is called homogeneous if $a_0(x) = 0$.

If $S_1(x), S_2(x) \dots $ are solution to the homogeneous equation, then any of its linear combinations are also a solution.
\end{definition}

\begin{theorem}[Solutions to Linear First Order ODE]
The linear ODE of the form:
\[
	y' + P(x)y = Q(x)
\]
has the general solution:
\[
	y(x) = e^{-\int P(x) dx}\left( \int Q(x) e^{\int P(x) dx} dx + C\right)
\]
Where $C$ is an arbitrary constant.

\end{theorem}

\begin{theorem}[Separable ODE]
	ODE are separarble if it can be written in the form:
$
	\frac{dy}{dx} h(y) = g(x)
$
\end{theorem}

\begin{theorem}[Homogeneous ODE (Another Homogeneous)]
A ODE is called homogeneous if it can be written in the form:
\[
	\frac{dy}{dx}  = g(\frac{y}{x})
\]
It can be solved by introducing a new variable $u = \frac{y}{x}$. Notice 
$ \frac{dy}{dx} = u + x \frac{du}{dx}$.
\end{theorem}

\begin{theorem}[Exact ODE]
	The ODE is exact if it can be written in the form:
$
	\frac{ \partial \psi}{ \partial x} + \frac{ \partial \psi}{ \partial y} \frac{dy}{dx} = 0
$
. Where $\psi$ is a function of $x$ and $y$.
And its general the solution is $ \psi (x,y) = C$.

Moreover, a equation in the form of $ M(x,y) + \frac{dy}{dx} N(x,y) = 0$ is if and only if $ \frac{ \partial M}{y} = \frac{ \partial N}{x}.$
c.f. the determination of conservative vector field.
\end{theorem}

\begin{remark}[On Integration Factor]
Consider the following equation:
$
	3xy+y^2 +(x^2+xy)y' = 0
$
This equation can be turned into an exact equation by multiplying both sides by $x$, and the $\psi$ function can be found to be: $x^3y+ \frac{1}{2}x^2y^2.$
\end{remark}

\begin{definition}[Wroskian]
	The Wroskian of two functions, $f(x)$ and $g(x)$ is defined as:
	$
		W(f,g) = \det \begin{bmatrix} f(x) & g(x) \\ f'(x) & g'(x) \end{bmatrix}
	$
\end{definition}

\begin{theorem}[General Solutions to Second Order Homogeneous ODE with Constant Coefficients] \label{SolutionSOODEConstant}
	Let $y'' + a_1y' +  a_2y = 0$ be a second order homogeneous ODE.
	Substitute the function $T(x) = e^{rx},$ where $r$ is a constant:
\begin{equation}\label{eq:2HOMOODE}
		r^2e^{rx} + a_1re^{rx} + a_2e^{rx} = 0 \iff r^2 + a_1r + a_2 = 0
\end{equation}

\textbf{Case 1}: equation \ref{eq:2HOMOODE} has two distinct real roots $r_1$ and $r_2$.
The general solution is: 
$
	y(x) = C_1e^{r_1x} + C_2e^{r_2x}
$

\textbf{Case 2}: equation \ref{eq:2HOMOODE} has two repeated real roots $r_1$.
The general solution is:
$
	y(x) = C_1e^{r_1x} + C_2x e^{r_1x}
$ 

\textbf{Case 3}: equation \ref{eq:2HOMOODE} has two complex roots $r_1+im_1$ and $r_1 - im_1$, where $i$ is the imaginary unit.

Recall Euler's formula: $e^{ix} = \cos x + i \sin x$.
Then the general solution is:
$ 
	y(x) = C_1e^{r_1x}e^{im_1x} + C_2e^{r_1x}e^{-im_1x}
$
Which is equivalent to:
$
	y(x) = C_3\cos(m_1x)e^{r_1x} + C_4\sin(m_1x)e^{r_1x}
$
\end{theorem}

\begin{theorem}[Method of Variation of Parameters]
	Consider the linear non-homogeneous second order ODE:
\begin{equation}\label{eq:2NHOMOODE}
	ay'' + by' + cy = g(t)
\end{equation}
Suppose the homogeneous equation ($ay'' + by' + cy = 0$) has the general solution:
$
	y_c(t) = C_1y_1(t) + C_2y_2(t)
$
Then the particular solution of the non-homogeneous equation \ref{eq:2NHOMOODE} is: 
\[
	y_p(t) = -y_1(t)\int \frac{y_2(t)g(t)}{aW(y_1,y_2)} dt + y_2(t)\int \frac{y_1(t)g(t)}{aW(y_1,y_2)} dt
\]

Where $W$ is Wronskian operator and 
$W(y_1,y_2) = y_1y_2' - y_2y_1'$.
\end{theorem}

\begin{theorem}[Solution to Euler's Equation]
	Consider Euler's equation:
$
	ax^2 \frac{d^2y}{dx^2} + bx \frac{dy}{dx} + cy = 0
$
for constants $a,b,c$. The solution of can be found by introducing a new variable $t = ln(x)$
Notice $ \frac{dy}{dx} = \frac{dy}{dt} \frac{dt}{dx} = \frac{dy}{dt} \frac{1}{x}$
and $ \frac{d^2y}{dx^2} = \frac{d^2y}{dt^2} \frac{1}{x^2} - \frac{dy}{dt} \frac{1}{x^2}$

Then the equation becomes:
$
	a\frac{d^2y}{dt^2} + (b-a)\frac{dy}{dt} + cy = 0
$
Which can be solved by theorem \ref{SolutionSOODEConstant}.
\end{theorem}
\section{Quaedam Formulae Utillimae}
\subsection{Trig}
\begin{enumerate}
	\item $\sin^2{\theta} + \cos^2{\theta} = 1$
	\item $\cos(2\theta) = \cos^2{\theta} - \sin^2{\theta} = 2\cos^2{\theta} - 1$
	\item $\sin(2\theta) = 2\sin{\theta}\cos{\theta}$
	\item $\tan(2\theta) = \frac{2\tan{\theta}}{1-\tan^2{\theta}}$
	\item $\cot(2\theta) = \frac{2\tan{\theta}}{1+\tan^2{\theta}}$
\end{enumerate}

\begin{enumerate}
	\item $\int \cos^2{\theta} = \frac{\theta}{2} + \frac{\sin{2\theta}}{4}, \int_0^{\pi} \cos^2{\theta} = \frac{\pi}{2} $
	\item $\int \sin^2{\theta} = \frac{\theta}{2} - \frac{\sin{2\theta}}{4},  \int_0^{\pi} \sin^2{\theta} = \frac{\pi}{2} $
	\item For $ \underline{r}(\phi, \theta) = 
		a(\sin{\phi}\cos{\theta} \underline{i} +
		\sin{\phi}\sin{\theta} \underline{j} + 
		\cos{\phi} \underline{k}), 
		|\underline{r}_{\phi} \times \underline{r}_{\theta}| = 
		a^2\sin{\phi}$.
	\item For function $g(x,y)$, $|g_x \times g_y| = $ 


\end{enumerate}

\end{document}
