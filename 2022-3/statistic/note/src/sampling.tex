\section{Population and Sample}
\definition[Population, Sample, Censue]{
	\ 
	\begin{enumerate}
		\item \textbf{Population} is the set of all objects of interest in a statistical study.
		\item \textbf{Sample} is a subset of the population.
		\item \textbf{Census} is a sample that consists of the entire population (exactly once).
	\end{enumerate}
}

\definition[Representative and Repeated Sample]{
	A sample from population is said to be representative if all member of the population hvae an equal chance of being selected.

	Repeated sample is the process of taking multiple samples from the same population.
}

\section{Estimator and Estimate}
\definition[Estimator and Estimate]{
	The Estimator is a random variable; The estimate is its realisation. 
	The estimator is denoted with $T$, and the estimate is denoted with $t$. 
	The intrinsic population parameter is denoted with $\theta$, the estimate of the parameter is denoted as $\hat{\theta}$
	\begin{enumerate}
		\item \textbf{Estimator} is some function of the sample random variable, $T=g(X_1, X_2, \cdots, X_n)$ used to estimate a population parameter $\theta$. The Estimator, being a transformation fo the sample random variable, is also a random variable.
		\item \textbf{Estimate} is the value of the estimator for a particular sample, $t=g(x_1, \cdots ,x_n)$. It is a realisation of the random variable $T$.
	\end{enumerate}
}

\definition[Unbiased Estimator]{
	An estimator $T$ is said to be unbiased if $\bb{E}[T]=\theta$. Otherwise it is biased. 

	An estimator is asymptotically unbiased if $\bb{E}[T]\rightarrow \theta$ as $n\rightarrow \infty$.
}

\definition[Consistent Estimator]{
	An estimator $T$ is said to be consistent if 
\begin{enumerate}
	\item $T\rightarrow \theta$ as $n\rightarrow \infty$. (It is asymptotically unbiased)
	\item $\var{T}\rightarrow 0$ as $n\rightarrow \infty$.
\end{enumerate}
}

\definition[Sample Mean and Variance Estimator]{
	The sample mean and variance estimators are functions of sample random variables used to estimate the mean of variance of the population.
 	\begin{enumerate}
		\item The sample mean estimator is $\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i$.
		\item The sample variance estimator is $S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})^2$.
	\end{enumerate}
}

\theorem{
	The sample mean estimator is unbiased and consistent. $\var{\bar{E}}=\frac{\sigma^2}{n}$. The sample variance estimator is unbiased but not consistent.
}

\definition[Standard Error of The Estimator]{
	The standard error of an estimator is the square root of its variance.
$\ste{\bar{X}}=\frac{\sigma}{\sqrt{n}}$
}

\section{Constucting Estimators}
\subsection{Method of Moments}
\definition{
Consider some (population) random variable $X$ which has the probability distribution function $f_X(x|\theta)$ depending on some population parameter $\theta$. For $k = 1,2,3, \cdots, n$ the $k^{th}$ \textit{population moment} is defined as the expectation:
\begin{equation}
 \bb{E}[X^k] = \int x^k f_X(x|\theta) dx
\end{equation}

Let random variables $X_1, X_2, \cdots, X_n$ be a random sample from the population. The $k^{th}$ \textit{sample moment} is defined as:
\begin{equation}
	\bar{X^k} = \frac{1}{n}\sum_{i=1}^n X_i^k 
\end{equation}

}

\theorem{
	Let $n$ be population, $s^2$ be sample variance and $\bar{x}$ be sample mean, i.e., first sample moment. 
	Then the observed second sample moment is $\frac{n-1}{n}s^2+\bar{x}^2$

	The method of moment estimator may be biased or un biased.
}

\subsection{Maximum Likelihood Estimator}

\section{Interval Estimation}

\definition[Confidence Interval]{
	A 95\% confidence interval for a population parameter $\theta$ is an interval $[L, U]$ such that
	\begin{equation}
		\bb{P}(L\leq \theta \leq U)=0.95
	\end{equation}
	Confidence interval of 95\% is a interval that contains the parameter with probability 0.95; it is \emph{not} that the parameter has a 95\% chance of being in the interval.
}

\subsection{Normal Distribution With Known Variance}

For $n$ iid random variables $X_1, \cdots, X_n \sim N(\mu, \sigma^2)$, where variance is known, its mean estimator $\bar{X}$ is also normally distributed and $\bar{X} \sim N(\mu, \frac{\sigma^2}{n})$. Notice $Z=\frac{\bar{X}-\mu}{\sqrt{\sigma^2/ n}} \sim N(0,1)$. We call it $Z$ distribution. 

\begin{multline}
	0.95 = \bb{P}(-1.96\leq Z \leq 1.96) = \bb{P}(-1.96\leq \frac{\bar{X}-\mu}{\sqrt{\sigma^2/n}} \leq 1.96)\\ = 
	\bb{P}(\bar{X}-1.96\frac{\sigma}{\sqrt{n}}\leq \mu \leq \bar{X}+1.96\frac{\sigma}{\sqrt{n}})
\end{multline}

1.96 is the 97.5\% quantile of the standard normal distribution, and is denated as $z_{\alpha/2}$, where $\alpha = 0.05$.

$z_{0.025}$ can be found by the R command \texttt{qnorm(0.975, mean = 0, sd = 1)}.

Indeed, we can relax the restriction and using central limit theorm.

Suppose that $X_1, \cdots X_n$ are iid random variable with known variance $\sigma^2$. Then for large $n$, the sample mean $\bar{X}$ is approximately normally distributed with mean $\mu$ and variance $\frac{\sigma^2}{n}$. 

Thus, the $100(1-\alpha)$ confidence interval for $\mu$ is given by 
\begin{equation}
	(\bar{X} - z_{\alpha/2}\frac{\sigma}{\sqrt{n}}, \bar{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{n}})
\end{equation}

Notice $\ste{\bar{X}} = \frac{\sigma}{\sqrt{n}}$. 

Indeed, if the sample number is big enough, the variance can be estimated by the varaince estimator.


\subsection{Small Population, Chi-Square and t-Distribution}

For small population with unknown variance, we can use the chi-square distribution and t-distribution. 

The confidence interval for $\mu$ is given by

\begin{equation}
	(\bar{x} - t_{n-1, \alpha/2}\frac{s}{\sqrt{n}}, \bar{x} + t_{n-1, \alpha/2}\frac{s}{\sqrt{n}})
\end{equation}

where $t_{n-1, \alpha/2}$ is the $100(1-\alpha/2)$ quantile of the t-distribution with $n-1$ degrees of freedom, which can be found by the R command \texttt{qt(1-alpha/2, df = n-1)}.

The confidence interval for $\sigma^2$ is given by

\begin{equation}
	(\frac{(n-1)s^2}{\chi^2_{n-1, \alpha/2}}, \frac{(n-1)s^2}{\chi^2_{n-1, 1-\alpha/2}})
\end{equation}

where $\chi^2_{n-1, \alpha/2}$ and $\chi^2_{n-1, 1-\alpha/2}$ are the $100\alpha/2$ and $100(1-\alpha/2)$ quantiles of the chi-square distribution with $n-1$ degrees of freedom, which can be found by the R command \texttt{qchisq(alpha/2, df = n-1)} and \texttt{qchisq(1-alpha/2, df = n-1)}.

$\bar{x}, s^2$ are the sample mean and sample variance estimate.
